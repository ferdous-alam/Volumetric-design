
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>volumetric-design</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>
    <meta property="og:title" content="Representation Learning for Sequential Volumetric Design Tasks" />
    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Representation Learning for Sequential Volumetric Design Tasks" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />


<link rel="icon" href="img/D_10_S_119_5.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Representation Learning for Sequential Volumetric Design Tasks</br> 
                <small>
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ferdous-alam.github.io/">
                          Md Ferdous Alam
                        </a>
                        </br>The Ohio State University
                    </li>
                    <li>
                        <a href="">
                            Yi Wang
                        </a>
                        </br>Autodesk
                    </li>
                    <li>
                        <a href="">
                        Linh Tran
                        </a>
                        </br>Imperial College London
                    </li><br>
                    <li>
                        <a href="">
                        Chin-Yi Cheng
                        </a>
                        </br>Google
                    </li>
                    <li>
                        <a href="">
                        Jieliang Luo
                        </a>
                        </br>Autodesk
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2309.02583">
                            <image src="img/paper_image.png" height="120px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="TODO">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <image src="img/teaser_image.jpeg"height="500px">
            </div>

            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    Downstream applications for representation learning in volumetric design tasks
                </p>
            </div>
        </div>
        


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Architectural design tasks are essentially sequential in nature. Based on this intuition, here we propose an alternative perspective on data-driven automation in architectural design tasks. Our key idea is to encode the design knowledge from a collection of expert or high-permorning sequential designs and extract useful representations. Later we propose to utilize those learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. Specifically, we demonstrate our idea by leveraging a novel dataset of sequential volumetric designs. Volumetric design, also called massing design, is the first and critical step in professional building design. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. To improve the efficiency and accuracy of volumetric design evaluation, we propose a learning-based approach to create a preference model trained only from high-performing building design solutions. We assume the high-performing designs should have inherent similarities and that we can capture these similarities by extracting features from the high-performing solutions. The preference model can then compare two arbitrarily given volumetric designs. In addition, not only can our model evaluate static solutions but also the design process as the model can take time-series data as input. Finally, we develop an autoregressive model to generate volumetric designs from a partial design input and discuss its capabilities using the novel dataset. 
                </p>
            </div>
        </div>

        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="img/model_v2_color.jpeg" class="centerImage" height="400"></image>
                
                <!-- <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/S_0_T_5_GT.png" type="video/mp4" />
                </video> -->
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
							Learning a preference model from high performance design sequences
							</p>
						</div>
        </div>





        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <div class="col-md-8 col-md-offset-2 text-center">
                    <image src="img/preference_score-1.png" height="400"></image>
                    <image src="img/sequential_FID_score-1.png" height="300"></image>    
                </div>

        </div>





        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                Thanks to Ricardo Martin-Brualla and David Salesin for their comments on the text, and to George Drettakis and Georgios Kopanas for graciously assisting us with our baseline evaluation.
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div> -->
</body>
</html>
